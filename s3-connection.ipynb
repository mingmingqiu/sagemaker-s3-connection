{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ed9f78a-ecd8-49d0-ae53-68ae7f3b3238",
   "metadata": {},
   "source": [
    "<h1> Connection to s3 </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "execution_state": "idle",
   "id": "101e14ad-eda0-40f4-9e41-faa913d3b579",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T00:27:14.521730Z",
     "iopub.status.busy": "2025-12-06T00:27:14.521416Z",
     "iopub.status.idle": "2025-12-06T00:27:15.250896Z",
     "shell.execute_reply": "2025-12-06T00:27:15.249842Z",
     "shell.execute_reply.started": "2025-12-06T00:27:14.521709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset/bank-additional-full.csv\n",
      "Dataset/bank-additional-names.txt\n",
      "Dataset/bank-additional.csv\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import s3fs\n",
    "\n",
    "bucket = \"feature-engineering-bucket-xxxxxx\"\n",
    "prefix = \"Dataset/\"\n",
    "\n",
    "# List all files in the Dataset folder\n",
    "s3 = boto3.client('s3')\n",
    "response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "for obj in response.get('Contents', []):\n",
    "    print(obj[\"Key\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb45916-9d27-4a90-b1dc-af1314801cb5",
   "metadata": {},
   "source": [
    "boto3 is used to connect to s3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f493d2-851e-4f8f-8b99-0762b77f9e0c",
   "metadata": {},
   "source": [
    "<h1> Docker based preprocessing job</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "execution_state": "idle",
   "id": "de531626-a200-4c54-be44-846c82897854",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T02:20:49.128739Z",
     "iopub.status.busy": "2025-12-07T02:20:49.128406Z",
     "iopub.status.idle": "2025-12-07T02:26:15.930454Z",
     "shell.execute_reply": "2025-12-07T02:26:15.929446Z",
     "shell.execute_reply.started": "2025-12-07T02:20:49.128713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.ProcessingJob.NetworkConfig.VpcConfig.Subnets\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.ProcessingJob.NetworkConfig.VpcConfig.SecurityGroupIds\n",
      "..................\u001b[34mFeature engineering complete. Saved to: /opt/ml/processing/output/bank_additional_transformed.csv\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Role ARN - Replace this with your role ARN\n",
    "# role = \"arn:aws:iam::878254733488:role/sagemaker-role\"\n",
    "role = get_execution_role()\n",
    "\n",
    "# Define your custom image URI from ECR\n",
    "image_uri = \"961807745392.dkr.ecr.ap-south-1.amazonaws.com/sagemaker-feature-engineering:v2\"\n",
    "\n",
    "# Create a ScriptProcessor with the custom Docker image\n",
    "script_processor = ScriptProcessor(\n",
    "    image_uri=image_uri,\n",
    "    role=role,\n",
    "    command=[\"python3\"],  # Specify the command to run Python scripts\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.t3.medium\",\n",
    "    base_job_name=\"feature-engineering-job\"\n",
    ")\n",
    "\n",
    "# Run the script processor job\n",
    "script_processor.run(\n",
    "    code=\"feature_engineering.py\",  # Ensure this script is correctly located\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=\"s3://feature-engineering-bucket-989220949c9c/Dataset/bank-additional-full.csv\",  # S3 input file\n",
    "            destination=\"/opt/ml/processing/input\"  # Path inside the container\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            source=\"/opt/ml/processing/output\",  # Path inside the container\n",
    "            destination=\"s3://feature-engineering-bucket-989220949c9c/output/\"  # S3 output location\n",
    "        )\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520fadfe-2fbb-421b-821c-bc507816bdcc",
   "metadata": {},
   "source": [
    "in sagemaker job, docker provides the execution environment through the defintiion of script_processor. Script processor defines the script to be run in this environment, and the input \"/opt/ml/processing/input\" and the output \"/opt/ml/processing/output\". Even thorugh maybe feature_engineering.py may be defined in Dockerfile, how the script mentioned in the processor job script_processor will overwrite the same file defined during the docker definition."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
